{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from analysis.core import Core, calculate_B_C, evaluate_quantum_results_with_uncertainties, build_histograms, \\\n",
    "    build_results\n",
    "import vector\n",
    "\n",
    "vector.register_awkward()\n",
    "import awkward as ak"
   ],
   "id": "398a62645dbe12c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T11:51:52.886272Z",
     "start_time": "2025-06-27T11:51:50.480125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# file_tag = 'pretrain-ema-ds1p0'\n",
    "file_tag = 'scratch-ema-ds1p0-lr_half'\n",
    "\n",
    "raw_data = torch.load(f'data/{file_tag}.pt')"
   ],
   "id": "4918aa5a4a748bd6",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Truth Analysis\n",
    "\n",
    "- `t  -> W+  b,     W+  -> l+  v`\n",
    "- `t~ -> W-  b~,    W-  -> l-  v~`\n",
    "\n",
    "---\n",
    "\n",
    "extra truth variables columns: `[pt, eta, phi, mass, energy, index, mother1, mother2]`"
   ],
   "id": "e4ee76f2096d8adf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T11:51:56.491608Z",
     "start_time": "2025-06-27T11:51:53.826950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_truth = {\n",
    "    key.replace('EXTRA/lhe/', ''): torch.cat([item[key] for item in raw_data]).numpy()\n",
    "    for key in raw_data[0].keys()\n",
    "    if key.startswith('EXTRA/lhe/')\n",
    "}\n",
    "\n",
    "\n",
    "def sanity_and_merge(pairs, data):\n",
    "    merged = {}\n",
    "    for a, b, new_key in pairs:\n",
    "        valid_a = ~np.isnan(data[a]).all(axis=1)\n",
    "        valid_b = ~np.isnan(data[b]).all(axis=1)\n",
    "        if np.any(valid_a & valid_b):\n",
    "            raise ValueError(f\"Conflict: both {a} and {b} present in same event.\")\n",
    "        valid_a = valid_a[:, None]  # broadcast\n",
    "        merged[new_key] = np.where(valid_a, data[a], data[b])\n",
    "    return merged\n",
    "\n",
    "\n",
    "# Define pairs to merge: (key_a, key_b, merged_key)\n",
    "pairs_to_merge = [\n",
    "    ('e+', 'mu+', 'l+'),\n",
    "    ('e-', 'mu-', 'l-'),\n",
    "    ('nu(e)', 'nu(mu)', 'v'),\n",
    "    ('nu(e)~', 'nu(mu)~', 'v~'),\n",
    "]\n",
    "\n",
    "# Run merging with sanity check\n",
    "truth_data = sanity_and_merge(pairs_to_merge, raw_truth)\n",
    "for k in ['W+', 'W-', 'b', 'b~', 't', 't~']:\n",
    "    truth_data[k] = raw_truth[k]\n",
    "\n",
    "for k in truth_data.keys():\n",
    "    truth_data[k] = vector.zip({\n",
    "        'pt': truth_data[k][:, 0],\n",
    "        'eta': truth_data[k][:, 1],\n",
    "        'phi': truth_data[k][:, 2],\n",
    "        'mass': truth_data[k][:, 3],\n",
    "    })\n",
    "\n",
    "truth_core = Core(\n",
    "    main_particle_1=truth_data['t'],\n",
    "    main_particle_2=truth_data['t~'],\n",
    "    child1=truth_data['l+'],\n",
    "    child2=truth_data['l-'],\n",
    ")\n",
    "\n",
    "truth_result = truth_core.analyze()\n",
    "truth_result = truth_result.query('mass < 400')\n",
    "truth_hists = build_histograms(truth_result)\n",
    "\n",
    "result, result_up, result_down = calculate_B_C(truth_hists, kappas=(1.0, -1.0))\n",
    "D = -(result['C_nn'] + result['C_rr'] + result['C_kk'])\n",
    "final = evaluate_quantum_results_with_uncertainties(result, result_up, result_down)"
   ],
   "id": "d1b4ee49d4233586",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Recon Analysis\n",
    "\n",
    "### point cloud structure\n",
    "\n",
    "- `full_input_point_cloud`: (N, 18, 7) array of particles in the event\n",
    "    - columns: `[energy, pt, eta, phi, btag, isLepton, charge]`\n",
    "- `t1 > b1, l1` l1 is 11 and 13\n",
    "- `t2 > b2, l2` l2 is -11 and -13\n",
    "\n",
    "> From LHE, lepton sign is correct, which means `l+` is for e+ and muon+; while for assignment, `l+` is for e- and muon-"
   ],
   "id": "8a8bfe86df30be04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T11:52:06.148221Z",
     "start_time": "2025-06-27T11:51:59.137508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def classify_TT2L(point_cloud, assignment_target, event_selection=None):\n",
    "    \"\"\"\n",
    "    Classify particles in a TT2L topology.\n",
    "\n",
    "    Parameters:\n",
    "        point_cloud: (N, num_particles, num_features)\n",
    "        assignment_target: tuple/list with 2 index arrays (each (N, 2)) for the two groups\n",
    "\n",
    "    Returns:\n",
    "        Dict with b1, b2, l1, l2 reconstructions.\n",
    "        :param event_selection:\n",
    "    \"\"\"\n",
    "\n",
    "    idx = np.arange(point_cloud.shape[0])[:, None]\n",
    "\n",
    "    # Two targets (e.g., top1 and top2)\n",
    "    t1_target = assignment_target[0]  # (N, 2)\n",
    "    t2_target = assignment_target[1]  # (N, 2)\n",
    "\n",
    "    # Gather candidates\n",
    "    t1_recon_tmp = point_cloud[idx, t1_target, :].numpy()  # (N, 2, F)\n",
    "    t2_recon_tmp = point_cloud[idx, t2_target, :].numpy()\n",
    "\n",
    "    N = t1_recon_tmp.shape[0]\n",
    "\n",
    "    if event_selection is None:\n",
    "        event_selection = np.ones(N, dtype=bool)\n",
    "\n",
    "    def select_object(recon_tmp, mask_feature_idx, threshold=0.5):\n",
    "        \"\"\"\n",
    "        For each event, select the candidate if feature > threshold.\n",
    "        At most one is expected to be True. Return (N, F) with NaN for none.\n",
    "        \"\"\"\n",
    "        mask = recon_tmp[:, :, mask_feature_idx] > threshold  # (N, 2)\n",
    "        idx_first = np.argmax(mask, axis=1)  # (N,)\n",
    "        has_true = np.any(mask, axis=1)  # (N,)\n",
    "        valid = has_true & event_selection  # only keep events that pass both conditions\n",
    "\n",
    "        result = recon_tmp[np.arange(N), idx_first]  # (N, F)\n",
    "        result[~valid] = np.nan  # Set to NaN if no valid candidate or not selected\n",
    "\n",
    "        result = vector.zip({\n",
    "            'pt': np.expm1(result[:, 1]),\n",
    "            'eta': result[:, 2],\n",
    "            'phi': result[:, 3],\n",
    "            # 'mass': result[:, 4],\n",
    "            'energy': np.expm1(result[:, 0]),\n",
    "            'charge': result[:, 6],\n",
    "        })\n",
    "\n",
    "        return result\n",
    "\n",
    "    # B-jet: feature[4] > 0.5\n",
    "    b1_recon = select_object(t1_recon_tmp, 4)\n",
    "    b2_recon = select_object(t2_recon_tmp, 4)\n",
    "\n",
    "    # Lepton: feature[5] > 0.5\n",
    "    l1_recon = select_object(t1_recon_tmp, 5)\n",
    "    l2_recon = select_object(t2_recon_tmp, 5)\n",
    "\n",
    "    return {\n",
    "        'b1_recon': b1_recon,\n",
    "        'b2_recon': b2_recon,\n",
    "        'l1_recon': l1_recon,\n",
    "        'l2_recon': l2_recon,\n",
    "    }\n",
    "\n",
    "\n",
    "def zip_two_neutrinos(neutrino_dict):\n",
    "    def to_numpy(x):\n",
    "        return x.detach().cpu().numpy() if hasattr(x, 'detach') else np.asarray(x)\n",
    "\n",
    "    log_pt = to_numpy(neutrino_dict['log_pt'])  # (N, 2)\n",
    "    eta = to_numpy(neutrino_dict['eta'])  # (N, 2)\n",
    "    phi = to_numpy(neutrino_dict['phi'])  # (N, 2)\n",
    "\n",
    "    pt = np.expm1(log_pt)\n",
    "\n",
    "    nu1 = vector.zip({\n",
    "        \"pt\": pt[:, 0],\n",
    "        \"eta\": eta[:, 0],\n",
    "        \"phi\": phi[:, 0],\n",
    "        \"mass\": np.zeros_like(pt[:, 0]),  # neutrinos are massless\n",
    "    })\n",
    "\n",
    "    nu2 = vector.zip({\n",
    "        \"pt\": pt[:, 1],\n",
    "        \"eta\": eta[:, 1],\n",
    "        \"phi\": phi[:, 1],\n",
    "        \"mass\": np.zeros_like(pt[:, 1]),  # neutrinos are massless\n",
    "    })\n",
    "\n",
    "    return nu1, nu2\n",
    "\n",
    "\n",
    "def extract_batch_assignments(batch, classify_fn, process=\"TT2L\"):\n",
    "    pred = batch['assignment_prediction']\n",
    "    target = batch['assignment_target']\n",
    "    target_mask = batch['assignment_target_mask']\n",
    "\n",
    "    process_match = {\n",
    "        'num_lepton': batch['full_input_point_cloud'].sum(axis=1)[:, 5].numpy().astype(np.int32),\n",
    "        'num_bjet': batch['full_input_point_cloud'].sum(axis=1)[:, 4].numpy().astype(np.int32),\n",
    "        'total_charge': batch['full_input_point_cloud'].sum(axis=1)[:, 6].numpy().astype(np.int32),\n",
    "    }\n",
    "\n",
    "    common_selection = (\n",
    "            (process_match['num_bjet'] == 2) &\n",
    "            (process_match['num_lepton'] == 2) &\n",
    "            (process_match['total_charge'] == 0)\n",
    "    )\n",
    "\n",
    "    target_list = target[process]\n",
    "    pred_process = pred[process]['best_indices']\n",
    "    mask_process = target_mask[process]\n",
    "\n",
    "    process_match.update({\n",
    "        **classify_fn(batch['full_input_point_cloud'], target_list, event_selection=common_selection),\n",
    "    })\n",
    "\n",
    "    nu1_pred, nu2_pred = zip_two_neutrinos(batch['neutrinos']['predict'])\n",
    "    nu1_true, nu2_true = zip_two_neutrinos(batch['neutrinos']['target'])\n",
    "    process_match.update({\n",
    "        \"nu1_recon\": nu1_pred,\n",
    "        \"nu2_recon\": nu2_pred,\n",
    "        \"nu1_truth\": nu1_true,\n",
    "        \"nu2_truth\": nu2_true,\n",
    "    })\n",
    "\n",
    "    for p_idx, (assignment_target, assignment_prediction, assignment_target_mask) in enumerate(\n",
    "            zip(target_list, pred_process, mask_process)):\n",
    "        assignment_target = assignment_target.numpy()\n",
    "        assignment_prediction = assignment_prediction.numpy()\n",
    "        assignment_target_mask = assignment_target_mask.numpy()\n",
    "\n",
    "        # Matching: true if all particles in the group are correctly assigned\n",
    "        matched = (assignment_target == assignment_prediction)\n",
    "        matched = matched.all(axis=1)  # along particle axis\n",
    "\n",
    "        process_match[f\"{process}_{p_idx}\"] = matched\n",
    "        process_match[f\"{process}_{p_idx}_mask\"] = assignment_target_mask\n",
    "\n",
    "    return process_match\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for batch in raw_data:\n",
    "    out = extract_batch_assignments(batch, classify_fn=classify_TT2L)\n",
    "    dfs.append(out)\n",
    "\n",
    "# Instead of pd.concat, build one big awkward.Array\n",
    "recon_data = ak.zip({\n",
    "    k: ak.concatenate([out[k] for out in dfs])\n",
    "    for k in dfs[0].keys()\n",
    "})\n",
    "\n",
    "truth_particle = {\n",
    "    'b1': truth_data['b~'],\n",
    "    'b2': truth_data['b'],\n",
    "    'l1': truth_data['l-'],\n",
    "    'l2': truth_data['l+'],\n",
    "    't1': truth_data['t~'],\n",
    "    't2': truth_data['t'],\n",
    "}\n",
    "\n",
    "for p, v in truth_particle.items():\n",
    "    recon_data = ak.with_field(recon_data, v, f'{p}_truth')\n",
    "\n",
    "recon_data = ak.with_field(recon_data, recon_data.l1_recon + recon_data.nu1_recon + recon_data.b1_recon, f't1_recon')\n",
    "recon_data = ak.with_field(recon_data, recon_data.l2_recon + recon_data.nu2_recon + recon_data.b2_recon, f't2_recon')\n",
    "\n",
    "truth_result = Core(\n",
    "    main_particle_1=recon_data.t2_truth,\n",
    "    main_particle_2=recon_data.t1_truth,\n",
    "    child1=recon_data.l2_truth,\n",
    "    child2=recon_data.l1_truth,\n",
    ").analyze()\n",
    "\n",
    "recon_result = Core(\n",
    "    main_particle_1=recon_data.t2_recon,\n",
    "    main_particle_2=recon_data.t1_recon,\n",
    "    child1=recon_data.l2_recon,\n",
    "    child2=recon_data.l1_recon,\n",
    ").analyze()\n",
    "\n",
    "full = build_results(truth_result, recon_result)"
   ],
   "id": "8e945d98a87631a6",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T11:52:07.401632Z",
     "start_time": "2025-06-27T11:52:07.268103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_recon_cut = (recon_data[\"num_bjet\"] == 2) & (recon_data[\"num_lepton\"] == 2) & (recon_data[\"total_charge\"] == 0)\n",
    "mask_all_true = (\n",
    "        recon_data[\"TT2L_0\"] &\n",
    "        recon_data[\"TT2L_0_mask\"] &\n",
    "        recon_data[\"TT2L_1\"] &\n",
    "        recon_data[\"TT2L_1_mask\"]\n",
    "        & base_recon_cut\n",
    ")\n",
    "\n",
    "valid_events = (\n",
    "        base_recon_cut &\n",
    "        recon_data.TT2L_0_mask &\n",
    "        recon_data.TT2L_1_mask\n",
    ")\n",
    "\n",
    "# Count how many events have all four True\n",
    "count = ak.sum(mask_all_true)\n",
    "print(\"Number of events with all four True:\", count, \"out of\", ak.sum(valid_events), \"percentage:\",\n",
    "      count / ak.sum(valid_events) * 100)"
   ],
   "id": "8b887250c65968f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events with all four True: 172681 out of 241986 percentage: 71.35991338341887\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Unfolding",
   "id": "e7e270e66d8cb961"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T11:55:21.873901Z",
     "start_time": "2025-06-27T11:52:09.378748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from analysis.unfold import main\n",
    "\n",
    "bin_nums = 20 + 1\n",
    "bin_edges = {\n",
    "    \"m_tt\": np.array([0, 400, 500, 800, np.inf]),\n",
    "    \"B_Ak\": np.linspace(-1, 1, bin_nums),\n",
    "    \"B_An\": np.linspace(-1, 1, bin_nums),\n",
    "    \"B_Ar\": np.linspace(-1, 1, bin_nums),\n",
    "    \"B_Bk\": np.linspace(-1, 1, bin_nums),\n",
    "    \"B_Bn\": np.linspace(-1, 1, bin_nums),\n",
    "    \"B_Br\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_kk\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_kn\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_kr\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_nk\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_nn\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_nr\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_rk\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_rn\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_rr\": np.linspace(-1, 1, bin_nums),\n",
    "}\n",
    "\n",
    "full['weight'] = 19.29e3 * 140 / len(full)\n",
    "unfolded = main(full, bin_edges=bin_edges, weight_col='weight')\n",
    "\n",
    "# 1) Reshape all unfolded arrays for each variable (except m_tt)\n",
    "mtt_nbins = len(bin_edges['m_tt']) - 1\n",
    "\n",
    "unfolded_temp = {\n",
    "    key: {\n",
    "        'edges': edges,\n",
    "        'counts': unfolded[f'{key}_recon_unfold_content'].to_numpy().reshape(mtt_nbins, len(edges) - 1),\n",
    "        'errors': unfolded[f'{key}_recon_unfold_error'].to_numpy().reshape(mtt_nbins, len(edges) - 1),\n",
    "    }\n",
    "    for key, edges in bin_edges.items()\n",
    "    if key != 'm_tt'\n",
    "}\n",
    "\n",
    "# 2) Split by m_tt bins using clean dict comprehension\n",
    "unfolded_hists = {\n",
    "    f\"m_tt < {mtt_right}\": {\n",
    "        key: {\n",
    "            'edges': data['edges'],\n",
    "            'counts': data['counts'][idx],\n",
    "            'errors': data['errors'][idx],\n",
    "        }\n",
    "        for key, data in unfolded_temp.items()\n",
    "    }\n",
    "    for idx, mtt_right in enumerate(bin_edges['m_tt'][1:])\n",
    "}\n",
    "\n",
    "result, result_up, result_down = calculate_B_C(unfolded_hists['m_tt < 400.0'], kappas=(1.0, -1.0))\n",
    "D = -(result['C_nn'] + result['C_rr'] + result['C_kk'])\n",
    "final = evaluate_quantum_results_with_uncertainties(result, result_up, result_down)"
   ],
   "id": "ef68587162372992",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: recon - Variable: B_Ak - done\n",
      "Category: recon - Variable: B_An - done\n",
      "Category: recon - Variable: B_Ar - done\n",
      "Category: recon - Variable: B_Bk - done\n",
      "Category: recon - Variable: B_Bn - done\n",
      "Category: recon - Variable: B_Br - done\n",
      "Category: recon - Variable: C_kk - done\n",
      "Category: recon - Variable: C_kn - done\n",
      "Category: recon - Variable: C_kr - done\n",
      "Category: recon - Variable: C_nk - done\n",
      "Category: recon - Variable: C_nn - done\n",
      "Category: recon - Variable: C_nr - done\n",
      "Category: recon - Variable: C_rk - done\n",
      "Category: recon - Variable: C_rn - done\n",
      "Category: recon - Variable: C_rr - done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info in <TCanvas::Print>: pdf file plots/unfolding/B_Ak_recon.pdf has been created\n",
      "Info in <TCanvas::Print>: pdf file plots/unfolding/B_An_recon.pdf has been created\n",
      "Info in <TCanvas::Print>: pdf file plots/unfolding/B_Ar_recon.pdf has been created\n",
      "Info in <TCanvas::Print>: pdf file plots/unfolding/B_Bk_recon.pdf has been created\n",
      "Info in <TCanvas::Print>: pdf file plots/unfolding/B_Bn_recon.pdf has been created\n",
      "Info in <TCanvas::Print>: pdf file plots/unfolding/B_Br_recon.pdf has been created\n",
      "Info in <TCanvas::Print>: pdf file plots/unfolding/C_kk_recon.pdf has been created\n",
      "Info in <TCanvas::Print>: pdf file plots/unfolding/C_kn_recon.pdf has been created\n",
      "Info in <TCanvas::Print>: pdf file plots/unfolding/C_kr_recon.pdf has been created\n",
      "Info in <TCanvas::Print>: pdf file plots/unfolding/C_nk_recon.pdf has been created\n",
      "Info in <TCanvas::Print>: pdf file plots/unfolding/C_nn_recon.pdf has been created\n",
      "Info in <TCanvas::Print>: pdf file plots/unfolding/C_nr_recon.pdf has been created\n",
      "Info in <TCanvas::Print>: pdf file plots/unfolding/C_rk_recon.pdf has been created\n",
      "Info in <TCanvas::Print>: pdf file plots/unfolding/C_rn_recon.pdf has been created\n",
      "Info in <TCanvas::Print>: pdf file plots/unfolding/C_rr_recon.pdf has been created\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T11:55:56.029590Z",
     "start_time": "2025-06-27T11:55:55.917948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "base_df = pd.DataFrame({\n",
    "    'value': result,\n",
    "    'uncertainty_up': result_up,\n",
    "    'uncertainty_down': result_down\n",
    "})\n",
    "\n",
    "base_df['uncertainty_up'] = base_df['uncertainty_up'] - base_df['value']\n",
    "base_df['uncertainty_down'] = base_df['value'] - base_df['uncertainty_down']\n",
    "\n",
    "# Step 2: Combine with `final` entries (like 'Concurrence', 'Ckk + Cnn', etc.)\n",
    "final_df = pd.DataFrame.from_dict(final, orient='index')\n",
    "final_df.index.name = 'name'\n",
    "\n",
    "# Step 3: Concatenate both\n",
    "combined_df = pd.concat([base_df, final_df])\n",
    "\n",
    "# Optional: Reset index if you prefer flat structure\n",
    "combined_df.index.name = 'name'\n",
    "combined_df = combined_df.reset_index()\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_df.to_csv(f'{file_tag}.csv', index=False)"
   ],
   "id": "63e6899f04eb0cd1",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plotting",
   "id": "c0cf66bd09b3f3df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from downstreams.plotting.unfolding import plot_uncertainty_with_ratio\n",
    "from downstreams.plotting.kinematic_comparison import plot_kinematics_comparison\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "bins_mtt = bin_edges[\"m_tt\"]\n",
    "mtt_labels = [\n",
    "    r\"$m_{t\\bar{t}} < 400$\",\n",
    "    r\"$400 < m_{t\\bar{t}} < 500$\",\n",
    "    r\"$500 < m_{t\\bar{t}} < 800$\",\n",
    "    r\"$m_{t\\bar{t}} \\geq 800$\"\n",
    "]\n",
    "\n",
    "common_labels = {\n",
    "    # 6 B terms: B_{A,B}{n,r,k}\n",
    "    **{\n",
    "        f\"B_{which}{axis}\": {\n",
    "            \"name\": rf\"$\\cos\\theta^{{{which}}}_{{{axis}}}$\",\n",
    "            \"labels\": [f\"bin {i}\" for i in range(len(bin_edges[f\"B_{which}{axis}\"]) - 1)],\n",
    "        }\n",
    "        for which in ['A', 'B']\n",
    "        for axis in ['n', 'r', 'k']\n",
    "    },\n",
    "    # 9 C terms: C_{axis1}{axis2} for axis1, axis2 in {n,r,k}\n",
    "    **{\n",
    "        f\"C_{ax1}{ax2}\": {\n",
    "            \"name\": rf\"$\\cos\\theta^A_{{{ax1}}} \\cos\\theta^B_{{{ax2}}}$\",\n",
    "            \"labels\": [f\"bin {i}\" for i in range(len(bin_edges[f\"C_{ax1}{ax2}\"]) - 1)],\n",
    "        }\n",
    "        for ax1 in ['n', 'r', 'k']\n",
    "        for ax2 in ['n', 'r', 'k']\n",
    "    },\n",
    "}\n",
    "\n",
    "for var, var_cfg in common_labels.items():\n",
    "    methods = [\n",
    "        {\n",
    "            \"name\": r\"EveNet - Pretrain\", \"color\": \"green\",\n",
    "            # \"data\": unfolded[f\"{var}_recon_unfold_error\"]\n",
    "            \"data\": unfolded[f\"{var}_recon_unfold_content\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": r\"EveNet - Scratch\", \"color\": \"green\",\n",
    "            \"data\": unfolded[f\"{var}_recon_unfold_error\"]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    plot_uncertainty_with_ratio(\n",
    "        mtt_labels, var_cfg[\"labels\"], var_cfg['name'], methods,\n",
    "        ratio_baseline_name=r\"EveNet - Pretrain\",\n",
    "        p_dir=Path(os.getcwd()) / \"plots\",\n",
    "        save_name=f\"unfolded_{var}.pdf\",\n",
    "        ratio_baseline_max=0.25,\n",
    "        ratio_baseline_min=-0.05,\n",
    "        ratio_y_label=r\"Improvement to EveNet - Pretrain\",\n",
    "    )\n",
    "\n"
   ],
   "id": "b3a1375405ecdd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "p_dir = Path(os.getcwd()) / \"plots\"\n",
    "named_configs = {\n",
    "    \"neutrino\": {\n",
    "        \"variables\": [\"pt\", \"eta\", \"phi\"],\n",
    "        \"x_labels\": [r\"$p_T^{\\nu}$ [GeV]\", r\"$\\eta^{\\nu}$\", r\"$\\phi^{\\nu}$\"],\n",
    "        \"kin_range\": {\"pt\": (0, 350), \"eta\": (-np.pi * 1.5, np.pi * 1.5), \"phi\": (-np.pi, np.pi)},\n",
    "        # \"labels\": [r\"$\\nu$ from $(top^+)$\", r\"$\\nu$ from $(top^-)$\"],\n",
    "        \"labels\": [r\"$\\nu$ (scratch)\", r\"$\\nu$ (pretrain)\"],\n",
    "        \"colors\": ['#5bb5ac', '#de526c'],\n",
    "        \"columns\": ['nu'],\n",
    "        \"log_y\": [True, False, False],\n",
    "    },\n",
    "    \"top\": {\n",
    "        \"variables\": [\"pt\", \"eta\", \"phi\", \"mass\"],\n",
    "        \"x_labels\": [r\"$p_T^{t}$ [GeV]\", r\"$\\eta^{t}$\", r\"$\\phi^{t}$\", r\"$mass^{t}$ [GeV]\"],\n",
    "        \"kin_range\": {\"pt\": (0, 600), \"eta\": (-np.pi * 1.5, np.pi * 1.5), \"phi\": (-np.pi, np.pi), \"mass\": (100, 240)},\n",
    "        # \"labels\": [r\"$(top^+)$ \", r\"$(top^-)$\"],\n",
    "        \"labels\": [r\"$top$ (scratch)\", r\"$top$ (pretrain)\"],\n",
    "        \"colors\": ['#5bb5ac', '#de526c'],\n",
    "        \"columns\": ['t'],\n",
    "        \"log_y\": [True, False, False, False],\n",
    "    },\n",
    "    # \"W\": {\n",
    "    #     \"variables\": [\"pt\", \"eta\", \"phi\", \"mass\"],\n",
    "    #     \"x_labels\": [r\"$p_T^{W}$ [GeV]\", r\"$\\eta^{W}$\", r\"$\\phi^{W}$\", r\"$mass^{W}$ [GeV]\"],\n",
    "    #     \"kin_range\": {\"pt\": (0, 350), \"eta\": (-np.pi * 1.5, np.pi * 1.5), \"phi\": (-np.pi, np.pi), \"mass\": (40, 120)},\n",
    "    #     # \"labels\": [r\"$(W^+)$\", r\"$(W^-)$\"],\n",
    "    #     \"labels\": [r\"$W$ (scratch)\", r\"$W$ (pretrain)\"],\n",
    "    #     \"colors\": ['#5bb5ac', '#de526c'],\n",
    "    #     \"columns\": ['W', 'plot_truth_W'],\n",
    "    #     \"log_y\": [True, False, False, False],\n",
    "    # },\n",
    "    # \"ttbar\": {\n",
    "    #     \"variables\": [\"mass\"],\n",
    "    #     \"x_labels\": [r\"$mass^{t\\bar{t}}$ [GeV]\"],\n",
    "    #     \"kin_range\": {\"mass\": (350, 1000)},\n",
    "    #     # \"labels\": [r\"$(top^+)$ \", r\"$(top^-)$\"],\n",
    "    #     \"labels\": [r\"$t\\bar{t}$ (scratch)\", r\"t\\bar{t} (pretrain)\"],\n",
    "    #     \"colors\": ['#5bb5ac', '#de526c'],\n",
    "    #     \"columns\": ['ttbar', 'plot_truth_ttbar'],\n",
    "    #     \"log_y\": [False],\n",
    "    # },\n",
    "}\n",
    "\n",
    "effective_recon_data = recon_data[\n",
    "    base_recon_cut & (recon_data.l1_recon.pt > 0) & (recon_data.l2_recon.pt > 0) &\n",
    "    (recon_data.b1_recon.pt > 0) & (recon_data.b2_recon.pt > 0)\n",
    "    ]\n",
    "\n",
    "for particle, cfg in named_configs.items():\n",
    "\n",
    "    for i, var in enumerate(cfg[\"variables\"]):\n",
    "        fig, axs = plt.subplots(\n",
    "            3, 1, figsize=(10, 16),\n",
    "            gridspec_kw={'height_ratios': [3, 1, 2], 'hspace': 0.0},\n",
    "            sharex=True\n",
    "        )\n",
    "\n",
    "        plot_kinematics_comparison(\n",
    "            axs=axs,\n",
    "            # kin=[getattr(nu[cfg['columns'][0]][..., 0], var), getattr(nu[cfg['columns'][0]][..., 1], var)],\n",
    "            # truth_kin=[getattr(nu[cfg['columns'][1]][..., 0], var), getattr(nu[cfg['columns'][1]][..., 1], var)],\n",
    "            kin=[\n",
    "                ak.to_numpy(ak.concatenate([\n",
    "                    getattr(effective_recon_data[f'{cfg[\"columns\"][0]}1_recon'], var),\n",
    "                    getattr(effective_recon_data[f'{cfg[\"columns\"][0]}2_recon'], var)\n",
    "                ], axis=0))\n",
    "            ],\n",
    "            truth_kin=[\n",
    "                ak.to_numpy(ak.concatenate([\n",
    "                    getattr(effective_recon_data[f'{cfg[\"columns\"][0]}1_truth'], var),\n",
    "                    getattr(effective_recon_data[f'{cfg[\"columns\"][0]}2_truth'], var)\n",
    "                ], axis=0))\n",
    "            ],\n",
    "            bins=100,\n",
    "            kin_range=cfg[\"kin_range\"][var],\n",
    "            labels=cfg[\"labels\"],\n",
    "            colors=cfg[\"colors\"],\n",
    "            xlabel=cfg[\"x_labels\"][i],\n",
    "            normalize_col=cfg.get(\"normalize_col\", False),\n",
    "            log_z=cfg.get(\"log_z\", True),\n",
    "            log_y=cfg.get(\"log_y\", [False, False, False, False])[i],\n",
    "            c_percent=np.array([10, 100])\n",
    "        )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if not os.path.exists(p_dir / \"kinematics\"):\n",
    "            os.makedirs(p_dir / \"kinematics\")\n",
    "        plt.savefig(p_dir / \"kinematics\" / f\"{particle}_{var}.pdf\")\n",
    "        plt.close(fig)"
   ],
   "id": "893948ab6d37ca9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "effective_recon_data = recon_data[base_recon_cut & (recon_data.l1_recon.pt > 0) & (recon_data.l2_recon.pt > 0)]\n",
    "\n",
    "X = ak.to_numpy(ak.concatenate([\n",
    "    getattr(effective_recon_data[f'b1_recon'], 'pt'),\n",
    "    getattr(effective_recon_data[f'b2_recon'], 'pt')\n",
    "], axis=0))\n",
    "\n",
    "print(\"X has NaNs?\", np.isnan(X).any())\n",
    "print(\"X has inf?\", np.isinf(X).any())\n",
    "print(\"X type:\", type(X))"
   ],
   "id": "5d6b0a092f3db5c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "299d764a88036639",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
