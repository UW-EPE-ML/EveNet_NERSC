{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from analysis.core import Core, calculate_B_C, evaluate_quantum_results_with_uncertainties, build_histograms, \\\n",
    "    build_results\n",
    "import vector\n",
    "\n",
    "vector.register_awkward()\n",
    "import awkward as ak"
   ],
   "id": "398a62645dbe12c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "raw_data = torch.load('data/TT2L-pretrain.pt')",
   "id": "4918aa5a4a748bd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Truth Analysis\n",
    "\n",
    "- `t  -> W+  b,     W+  -> l+  v`\n",
    "- `t~ -> W-  b~,    W-  -> l-  v~`\n",
    "\n",
    "---\n",
    "\n",
    "extra truth variables columns: `[pt, eta, phi, mass, energy, index, mother1, mother2]`"
   ],
   "id": "e4ee76f2096d8adf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "raw_truth = {\n",
    "    key.replace('EXTRA/lhe/', ''): torch.cat([item[key] for item in raw_data]).numpy()\n",
    "    for key in raw_data[0].keys()\n",
    "    if key.startswith('EXTRA/lhe/')\n",
    "}\n",
    "\n",
    "\n",
    "def sanity_and_merge(pairs, data):\n",
    "    merged = {}\n",
    "    for a, b, new_key in pairs:\n",
    "        valid_a = ~np.isnan(data[a]).all(axis=1)\n",
    "        valid_b = ~np.isnan(data[b]).all(axis=1)\n",
    "        if np.any(valid_a & valid_b):\n",
    "            raise ValueError(f\"Conflict: both {a} and {b} present in same event.\")\n",
    "        valid_a = valid_a[:, None]  # broadcast\n",
    "        merged[new_key] = np.where(valid_a, data[a], data[b])\n",
    "    return merged\n",
    "\n",
    "\n",
    "# Define pairs to merge: (key_a, key_b, merged_key)\n",
    "pairs_to_merge = [\n",
    "    ('e+', 'mu+', 'l+'),\n",
    "    ('e-', 'mu-', 'l-'),\n",
    "    ('nu(e)', 'nu(mu)', 'v'),\n",
    "    ('nu(e)~', 'nu(mu)~', 'v~'),\n",
    "]\n",
    "\n",
    "# Run merging with sanity check\n",
    "truth_data = sanity_and_merge(pairs_to_merge, raw_truth)\n",
    "for k in ['W+', 'W-', 'b', 'b~', 't', 't~']:\n",
    "    truth_data[k] = raw_truth[k]\n",
    "\n",
    "for k in truth_data.keys():\n",
    "    truth_data[k] = vector.zip({\n",
    "        'pt': truth_data[k][:, 0],\n",
    "        'eta': truth_data[k][:, 1],\n",
    "        'phi': truth_data[k][:, 2],\n",
    "        'mass': truth_data[k][:, 3],\n",
    "    })\n",
    "\n",
    "truth_core = Core(\n",
    "    main_particle_1=truth_data['t'],\n",
    "    main_particle_2=truth_data['t~'],\n",
    "    child1=truth_data['l+'],\n",
    "    child2=truth_data['l-'],\n",
    ")\n",
    "\n",
    "truth_result = truth_core.analyze()\n",
    "truth_result = truth_result.query('mass < 400')\n",
    "truth_hists = build_histograms(truth_result)\n",
    "\n",
    "result, result_up, result_down = calculate_B_C(truth_hists, kappas=(1.0, -1.0))\n",
    "D = -(result['C_nn'] + result['C_rr'] + result['C_kk'])\n",
    "final = evaluate_quantum_results_with_uncertainties(result, result_up, result_down)"
   ],
   "id": "d1b4ee49d4233586"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Recon Analysis\n",
    "\n",
    "### point cloud structure\n",
    "\n",
    "- `full_input_point_cloud`: (N, 18, 7) array of particles in the event\n",
    "    - columns: `[energy, pt, eta, phi, btag, isLepton, charge]`\n",
    "- `t1 > b1, l1` l1 is 11 and 13\n",
    "- `t2 > b2, l2` l2 is -11 and -13\n",
    "\n",
    "> From LHE, lepton sign is correct, which means `l+` is for e+ and muon+; while for assignment, `l+` is for e- and muon-"
   ],
   "id": "8a8bfe86df30be04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def classify_TT2L(point_cloud, assignment_target):\n",
    "    \"\"\"\n",
    "    Classify particles in a TT2L topology.\n",
    "\n",
    "    Parameters:\n",
    "        point_cloud: (N, num_particles, num_features)\n",
    "        assignment_target: tuple/list with 2 index arrays (each (N, 2)) for the two groups\n",
    "\n",
    "    Returns:\n",
    "        Dict with b1, b2, l1, l2 reconstructions.\n",
    "    \"\"\"\n",
    "\n",
    "    idx = np.arange(point_cloud.shape[0])[:, None]\n",
    "\n",
    "    # Two targets (e.g., top1 and top2)\n",
    "    t1_target = assignment_target[0]  # (N, 2)\n",
    "    t2_target = assignment_target[1]  # (N, 2)\n",
    "\n",
    "    # Gather candidates\n",
    "    t1_recon_tmp = point_cloud[idx, t1_target, :].numpy()  # (N, 2, F)\n",
    "    t2_recon_tmp = point_cloud[idx, t2_target, :].numpy()\n",
    "\n",
    "    N = t1_recon_tmp.shape[0]\n",
    "\n",
    "    def select_object(recon_tmp, mask_feature_idx, threshold=0.5):\n",
    "        \"\"\"\n",
    "        For each event, select the candidate if feature > threshold.\n",
    "        At most one is expected to be True. Return (N, F) with NaN for none.\n",
    "        \"\"\"\n",
    "        mask = recon_tmp[:, :, mask_feature_idx] > threshold  # (N, 2)\n",
    "        idx_first = np.argmax(mask, axis=1)  # (N,)\n",
    "        has_true = np.any(mask, axis=1)  # (N,)\n",
    "        result = recon_tmp[np.arange(N), idx_first]  # (N, F)\n",
    "        result[~has_true] = np.nan\n",
    "\n",
    "        result = vector.zip({\n",
    "            'pt': np.expm1(result[:, 1]),\n",
    "            'eta': result[:, 2],\n",
    "            'phi': result[:, 3],\n",
    "            # 'mass': result[:, 4],\n",
    "            'energy': np.expm1(result[:, 0]),\n",
    "        })\n",
    "\n",
    "        return result\n",
    "\n",
    "    # B-jet: feature[4] > 0.5\n",
    "    b1_recon = select_object(t1_recon_tmp, 4)\n",
    "    b2_recon = select_object(t2_recon_tmp, 4)\n",
    "\n",
    "    # Lepton: feature[5] > 0.5\n",
    "    l1_recon = select_object(t1_recon_tmp, 5)\n",
    "    l2_recon = select_object(t2_recon_tmp, 5)\n",
    "\n",
    "    return {\n",
    "        'b1_recon': b1_recon,\n",
    "        'b2_recon': b2_recon,\n",
    "        'l1_recon': l1_recon,\n",
    "        'l2_recon': l2_recon,\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_batch_assignments(batch, classify_fn, process=\"TT2L\"):\n",
    "    pred = batch['assignment_prediction']\n",
    "    target = batch['assignment_target']\n",
    "    target_mask = batch['assignment_target_mask']\n",
    "\n",
    "    process_match = {\n",
    "        'num_lepton': batch['full_input_point_cloud'].sum(axis=1)[:, 5].numpy().astype(np.int32),\n",
    "        'num_bjet': batch['full_input_point_cloud'].sum(axis=1)[:, 4].numpy().astype(np.int32),\n",
    "    }\n",
    "\n",
    "    target_list = target[process]\n",
    "    pred_process = pred[process]['best_indices']\n",
    "    mask_process = target_mask[process]\n",
    "\n",
    "    process_match.update({\n",
    "        **classify_fn(batch['full_input_point_cloud'], target_list)\n",
    "    })\n",
    "\n",
    "    for p_idx, (assignment_target, assignment_prediction, assignment_target_mask) in enumerate(\n",
    "            zip(target_list, pred_process, mask_process)):\n",
    "        assignment_target = assignment_target.numpy()\n",
    "        assignment_prediction = assignment_prediction.numpy()\n",
    "        assignment_target_mask = assignment_target_mask.numpy()\n",
    "\n",
    "        # Matching: true if all particles in the group are correctly assigned\n",
    "        matched = (assignment_target == assignment_prediction)\n",
    "        matched = matched.all(axis=1)  # along particle axis\n",
    "\n",
    "        process_match[f\"{process}_{p_idx}\"] = matched\n",
    "        process_match[f\"{process}_{p_idx}_mask\"] = assignment_target_mask\n",
    "\n",
    "    return process_match\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for batch in raw_data:\n",
    "    out = extract_batch_assignments(batch, classify_fn=classify_TT2L)\n",
    "    dfs.append(out)\n",
    "\n",
    "# Instead of pd.concat, build one big awkward.Array\n",
    "recon_data = ak.zip({\n",
    "    k: ak.concatenate([out[k] for out in dfs])\n",
    "    for k in dfs[0].keys()\n",
    "})\n",
    "\n",
    "truth_particle = {\n",
    "    'b1': truth_data['b~'],\n",
    "    'b2': truth_data['b'],\n",
    "    'l1': truth_data['l-'],\n",
    "    'l2': truth_data['l+'],\n",
    "    't1': truth_data['t~'],\n",
    "    't2': truth_data['t'],\n",
    "}\n",
    "\n",
    "for p, v in truth_particle.items():\n",
    "    recon_data = ak.with_field(recon_data, v, f'{p}_truth')\n",
    "\n",
    "truth_result = Core(\n",
    "    main_particle_1=recon_data.t2_truth,\n",
    "    main_particle_2=recon_data.t1_truth,\n",
    "    child1=recon_data.l2_truth,\n",
    "    child2=recon_data.l1_truth,\n",
    ").analyze()\n",
    "\n",
    "recon_result = Core(\n",
    "    main_particle_1=recon_data.t2_truth,\n",
    "    main_particle_2=recon_data.t1_truth,\n",
    "    child1=recon_data.l2_recon,\n",
    "    child2=recon_data.l1_recon,\n",
    ").analyze()\n",
    "\n",
    "full = build_results(truth_result, recon_result)"
   ],
   "id": "8e945d98a87631a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Unfolding",
   "id": "e7e270e66d8cb961"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from analysis.unfold import main\n",
    "\n",
    "bin_nums = 10 + 1\n",
    "bin_edges = {\n",
    "    \"m_tt\": np.array([0, 400, 500, 800, np.inf]),\n",
    "    \"B_Ak\": np.linspace(-1, 1, bin_nums),\n",
    "    \"B_An\": np.linspace(-1, 1, bin_nums),\n",
    "    \"B_Ar\": np.linspace(-1, 1, bin_nums),\n",
    "    \"B_Bk\": np.linspace(-1, 1, bin_nums),\n",
    "    \"B_Bn\": np.linspace(-1, 1, bin_nums),\n",
    "    \"B_Br\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_kk\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_kn\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_kr\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_nk\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_nn\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_nr\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_rk\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_rn\": np.linspace(-1, 1, bin_nums),\n",
    "    \"C_rr\": np.linspace(-1, 1, bin_nums),\n",
    "}\n",
    "\n",
    "unfolded = main(full, bin_edges=bin_edges)\n",
    "\n",
    "# 1) Reshape all unfolded arrays for each variable (except m_tt)\n",
    "mtt_nbins = len(bin_edges['m_tt']) - 1\n",
    "\n",
    "unfolded_temp = {\n",
    "    key: {\n",
    "        'edges': edges,\n",
    "        'counts': unfolded[f'{key}_recon_unfold_content'].to_numpy().reshape(mtt_nbins, len(edges) - 1),\n",
    "        'errors': unfolded[f'{key}_recon_unfold_error'].to_numpy().reshape(mtt_nbins, len(edges) - 1),\n",
    "    }\n",
    "    for key, edges in bin_edges.items()\n",
    "    if key != 'm_tt'\n",
    "}\n",
    "\n",
    "# 2) Split by m_tt bins using clean dict comprehension\n",
    "unfolded_hists = {\n",
    "    f\"m_tt < {mtt_right}\": {\n",
    "        key: {\n",
    "            'edges': data['edges'],\n",
    "            'counts': data['counts'][idx],\n",
    "            'errors': data['errors'][idx],\n",
    "        }\n",
    "        for key, data in unfolded_temp.items()\n",
    "    }\n",
    "    for idx, mtt_right in enumerate(bin_edges['m_tt'][1:])\n",
    "}\n",
    "\n",
    "result, result_up, result_down = calculate_B_C(unfolded_hists['m_tt < 400.0'], kappas=(1.0, -1.0))\n",
    "D = -(result['C_nn'] + result['C_rr'] + result['C_kk'])\n",
    "final = evaluate_quantum_results_with_uncertainties(result, result_up, result_down)"
   ],
   "id": "ef68587162372992"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plotting",
   "id": "c0cf66bd09b3f3df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from downstreams.plotting.unfolding import plot_uncertainty_with_ratio\n",
    "\n",
    "bins_mtt = bin_edges[\"m_tt\"]\n",
    "mtt_labels = [\n",
    "    r\"$m_{t\\bar{t}} < 400$\",\n",
    "    r\"$400 < m_{t\\bar{t}} < 500$\",\n",
    "    r\"$500 < m_{t\\bar{t}} < 800$\",\n",
    "    r\"$m_{t\\bar{t}} \\geq 800$\"\n",
    "]\n",
    "\n",
    "common_labels = {\n",
    "    # 6 B terms: B_{A,B}{n,r,k}\n",
    "    **{\n",
    "        f\"B_{which}{axis}\": {\n",
    "            \"name\": rf\"$\\cos\\theta^{{{which}}}_{{{axis}}}$\",\n",
    "            \"labels\": [f\"bin {i}\" for i in range(len(bin_edges[f\"B_{which}{axis}\"]) - 1)],\n",
    "        }\n",
    "        for which in ['A', 'B']\n",
    "        for axis in ['n', 'r', 'k']\n",
    "    },\n",
    "    # 9 C terms: C_{axis1}{axis2} for axis1, axis2 in {n,r,k}\n",
    "    **{\n",
    "        f\"C_{ax1}{ax2}\": {\n",
    "            \"name\": rf\"$\\cos\\theta^A_{{{ax1}}} \\cos\\theta^B_{{{ax2}}}$\",\n",
    "            \"labels\": [f\"bin {i}\" for i in range(len(bin_edges[f\"C_{ax1}{ax2}\"]) - 1)],\n",
    "        }\n",
    "        for ax1 in ['n', 'r', 'k']\n",
    "        for ax2 in ['n', 'r', 'k']\n",
    "    },\n",
    "}\n",
    "\n",
    "for var, var_cfg in common_labels.items():\n",
    "    methods = [\n",
    "        {\n",
    "            \"name\": r\"EveNet - Pretrain\", \"color\": \"green\",\n",
    "            # \"data\": unfolded[f\"{var}_recon_unfold_error\"]\n",
    "            \"data\": unfolded[f\"{var}_recon_unfold_content\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": r\"EveNet - Scratch\", \"color\": \"green\",\n",
    "            \"data\": unfolded[f\"{var}_recon_unfold_error\"]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    plot_uncertainty_with_ratio(\n",
    "        mtt_labels, var_cfg[\"labels\"], var_cfg['name'], methods,\n",
    "        ratio_baseline_name=r\"EveNet - Pretrain\",\n",
    "        p_dir=Path(os.getcwd()) / \"plots\",\n",
    "        save_name=f\"unfolded_{var}.pdf\",\n",
    "        ratio_baseline_max=0.25,\n",
    "        ratio_baseline_min=-0.05,\n",
    "        ratio_y_label=r\"Improvement to EveNet - Pretrain\",\n",
    "    )\n",
    "\n"
   ],
   "id": "b3a1375405ecdd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "101486381c53ce1"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
