{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "import json\n",
    "import yaml\n",
    "from types import SimpleNamespace\n",
    "from helpers.utils import clean_and_append\n",
    "\n"
   ],
   "id": "9873cfa59d6bc6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "args = SimpleNamespace(\n",
    "    config_workflow_pretrain = \"config-EMA-old/workflow_data.yaml\",\n",
    "    config_workflow_scratch  = \"config-EMA-scratch-old/workflow_data.yaml\",\n",
    "    boostrap = 8\n",
    ")\n",
    "base_dir_pretrain = os.path.dirname(os.path.abspath(args.config_workflow_pretrain))\n",
    "base_dir_scratch = os.path.dirname(os.path.abspath(args.config_workflow_scratch))\n"
   ],
   "id": "4c1f33fbe5ea174e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "boostrap_summary = dict()\n",
    "for iboostrap in range(args.boostrap):\n",
    "    boostrap_summary[iboostrap] = dict()\n",
    "    config_workflow_file_pretrain = os.path.join(base_dir_pretrain, f\"control-boostrap-{iboostrap}.yaml\")\n",
    "    with open(config_workflow_file_pretrain, 'r') as f_boost:\n",
    "        control = yaml.safe_load(f_boost)\n",
    "        storedir = control[\"output\"][\"storedir\"]\n",
    "        gendir = clean_and_append(storedir, \"_hybrid\")\n",
    "        gendir_nosignal = clean_and_append(gendir, \"_no_signal\")\n",
    "        gen_performance_json = os.path.join(gendir, \"SR\", \"gen_performance.json\")\n",
    "        gen_performance_nosignal_json = os.path.join(gendir_nosignal, \"SR\", \"gen_performance.json\")\n",
    "        with open(gen_performance_json, 'r') as f_gen:\n",
    "            gen_performance = json.load(f_gen)\n",
    "        with open(gen_performance_nosignal_json, 'r') as f_gen:\n",
    "            gen_performance_nosignal = json.load(f_gen)\n",
    "\n",
    "        boostrap_summary[iboostrap] = {\n",
    "            \"pretrain-OS\": gen_performance,\n",
    "            \"pretrain-SS\": gen_performance_nosignal,\n",
    "        }\n",
    "\n",
    "    config_workflow_file_scratch = os.path.join(base_dir_scratch, f\"control-boostrap-{iboostrap}.yaml\")\n",
    "\n",
    "    with open(config_workflow_file_scratch, 'r') as f_boost:\n",
    "        control = yaml.safe_load(f_boost)\n",
    "        storedir = control[\"output\"][\"storedir\"]\n",
    "        gendir = clean_and_append(storedir, \"_hybrid\")\n",
    "        gendir_nosignal = clean_and_append(gendir, \"_no_signal\")\n",
    "        gen_performance_json = os.path.join(gendir, \"SR\", \"gen_performance.json\")\n",
    "        gen_performance_nosignal_json = os.path.join(gendir_nosignal, \"SR\", \"gen_performance.json\")\n",
    "        with open(gen_performance_json, 'r') as f_gen:\n",
    "            gen_performance = json.load(f_gen)\n",
    "        with open(gen_performance_nosignal_json, 'r') as f_gen:\n",
    "            gen_performance_nosignal = json.load(f_gen)\n",
    "\n",
    "        boostrap_summary[iboostrap].update({\n",
    "            \"scratch-OS\": gen_performance,\n",
    "            \"scratch-SS\": gen_performance_nosignal,\n",
    "        })\n",
    "\n"
   ],
   "id": "652b1001a729297b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "\n",
    "def plot_aftercut_statistics_separate(boostrap_summary, figsize=(6, 6), output_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Generate separate boxplots for each 'after cut' metric (cov, mmd, efficiency),\n",
    "    comparing gen-OS and gen-SS across all bootstrap runs.\n",
    "    Saves each plot as a separate PDF file.\n",
    "\n",
    "    Parameters:\n",
    "    - boostrap_summary (dict): The summary from bootstrap runs.\n",
    "    - figsize (tuple): Size of each individual figure.\n",
    "    - output_dir (str): Directory to save output PDFs.\n",
    "\n",
    "    Returns:\n",
    "    - summary_stats (dict): Dict with mean and variance for each metric and label.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare data\n",
    "    records = []\n",
    "    for ibootstrap, summary in boostrap_summary.items():\n",
    "        for label in summary:\n",
    "            if 'after cut' in summary[label]:\n",
    "                metrics = summary[label]['after cut']\n",
    "                records.append({\n",
    "                    'bootstrap': ibootstrap,\n",
    "                    'label': label,\n",
    "                    'cov': metrics.get('cov', np.nan),\n",
    "                    'mmd': metrics.get('mmd', np.nan),\n",
    "                    'efficiency': metrics.get('efficiency', np.nan),\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df_melted = df.melt(id_vars=['bootstrap', 'label'], value_vars=['cov', 'mmd', 'efficiency'],\n",
    "                        var_name='metric', value_name='value')\n",
    "\n",
    "    # Determine all labels and order\n",
    "    all_labels = sorted(df['label'].unique())\n",
    "    ordered_labels = sorted([l for l in all_labels if l.endswith('OS')]) + \\\n",
    "                     sorted([l for l in all_labels if l.endswith('SS')])\n",
    "    group_split_index = sum(1 for l in ordered_labels if l.endswith('OS'))\n",
    "\n",
    "    # Compute summary stats\n",
    "    summary_stats = {}\n",
    "    for metric in ['cov', 'mmd', 'efficiency']:\n",
    "        summary_stats[metric] = {}\n",
    "        for label in ordered_labels:\n",
    "            values = df[(df['label'] == label)][metric].dropna()\n",
    "            summary_stats[metric][label] = {\n",
    "                'mean': float(np.mean(values)),\n",
    "                'variance': float(np.var(values, ddof=1)),\n",
    "                'std': float(np.std(values, ddof=1)),\n",
    "            }\n",
    "\n",
    "    # Set color palette\n",
    "    method_palette = {'pretrain': '#1f77b4', 'scratch': '#ff7f0e'}\n",
    "\n",
    "    for metric in ['cov', 'mmd', 'efficiency']:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "        data = df_melted[df_melted['metric'] == metric]\n",
    "        data = data[data['label'].isin(ordered_labels)]\n",
    "\n",
    "\n",
    "        sns.boxplot(data=data, x='label', y='value', palette=palette,\n",
    "                    width=0.6, fliersize=0, order=ordered_labels, ax=ax)\n",
    "\n",
    "        handles = []\n",
    "        for i, label in enumerate(ordered_labels):\n",
    "            mean = summary_stats[metric][label]['mean']\n",
    "            std = summary_stats[metric][label]['std']\n",
    "\n",
    "            # Mean line and errorbar\n",
    "            ax.hlines(mean, i - 0.3, i + 0.3, colors='black', linestyles='dashed', linewidth=1.2)\n",
    "            ax.errorbar(i, mean, yerr=std, fmt='o', color='black',\n",
    "                        capsize=6, elinewidth=2.5, markeredgewidth=2.5, markersize=8)\n",
    "\n",
    "            handles.append(Line2D([0], [0], marker='o', color='black',\n",
    "                    label=f\"{label} ({mean:.3f} ± {std:.3f})\",\n",
    "                    markersize=7, linestyle='dashed'))\n",
    "\n",
    "        # Vertical separator between OS and SS\n",
    "        ax.axvline(x=group_split_index - 0.5, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "        # Non-breaking dash for nicer tick labels\n",
    "        safe_labels = [lbl.replace(\"-\", \"\\u2011\") for lbl in ordered_labels]\n",
    "        ax.set_xticks(range(len(ordered_labels)))\n",
    "        ax.set_xticklabels(safe_labels, rotation=15)\n",
    "\n",
    "        ax.set_title(f\"{metric} (after cut)\", fontsize=14, weight='bold')\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.legend(handles=handles, loc='upper right', frameon=True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        output_path = os.path.join(output_dir, f\"aftercut_{metric}.pdf\")\n",
    "        plt.savefig(output_path, bbox_inches='tight')\n",
    "        output_path = os.path.join(output_dir, f\"aftercut_{metric}.png\")\n",
    "        plt.savefig(output_path, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    return summary_stats\n"
   ],
   "id": "29ca037f85140538"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "\n",
    "def plot_aftercut_statistics_separate(boostrap_summary, figsize=(6, 6), output_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Generate separate boxplots for each 'after cut' metric (cov, mmd, efficiency),\n",
    "    comparing gen-OS and gen-SS across all bootstrap runs.\n",
    "    Saves each plot as a separate PDF and PNG file.\n",
    "\n",
    "    Parameters:\n",
    "    - boostrap_summary (dict): The summary from bootstrap runs.\n",
    "    - figsize (tuple): Size of each individual figure.\n",
    "    - output_dir (str): Directory to save output PDFs and PNGs.\n",
    "\n",
    "    Returns:\n",
    "    - summary_stats (dict): Dict with mean and variance for each metric and label.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare data with 'method' extracted from label\n",
    "    records = []\n",
    "    for ibootstrap, summary in boostrap_summary.items():\n",
    "        for label in summary:\n",
    "            if 'after cut' in summary[label]:\n",
    "                metrics = summary[label]['after cut']\n",
    "                # Extract method from label, assumes label contains 'pretrain' or 'scratch'\n",
    "                method = 'pretrain' if 'pretrain' in label else 'scratch'\n",
    "                records.append({\n",
    "                    'bootstrap': ibootstrap,\n",
    "                    'label': label,\n",
    "                    'method': method,\n",
    "                    'cov': metrics.get('cov', np.nan),\n",
    "                    'mmd': metrics.get('mmd', np.nan),\n",
    "                    'efficiency': metrics.get('efficiency', np.nan),\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # Melt keeping 'method'\n",
    "    df_melted = df.melt(id_vars=['bootstrap', 'label', 'method'],\n",
    "                        value_vars=['cov', 'mmd', 'efficiency'],\n",
    "                        var_name='metric', value_name='value')\n",
    "\n",
    "    # Determine all labels and order (OS first then SS)\n",
    "    all_labels = sorted(df['label'].unique())\n",
    "    ordered_labels = sorted([l for l in all_labels if l.endswith('OS')]) + \\\n",
    "                     sorted([l for l in all_labels if l.endswith('SS')])\n",
    "    group_split_index = sum(1 for l in ordered_labels if l.endswith('OS'))\n",
    "\n",
    "    # Compute summary stats per metric, label, and method\n",
    "    summary_stats = {}\n",
    "    for metric in ['cov', 'mmd', 'efficiency']:\n",
    "        summary_stats[metric] = {}\n",
    "        for label in ordered_labels:\n",
    "            for method in ['pretrain', 'scratch']:\n",
    "                values = df[(df['label'] == label) & (df['method'] == method)][metric].dropna()\n",
    "                summary_stats[metric][(label, method)] = {\n",
    "                    'mean': float(np.mean(values)) if len(values) > 0 else np.nan,\n",
    "                    'variance': float(np.var(values, ddof=1)) if len(values) > 1 else np.nan,\n",
    "                    'std': float(np.std(values, ddof=1)) if len(values) > 1 else np.nan,\n",
    "                }\n",
    "\n",
    "    # Define color palette for methods\n",
    "    method_palette = {'pretrain': '#1f77b4', 'scratch': '#ff7f0e'}\n",
    "\n",
    "    for metric in ['cov', 'mmd', 'efficiency']:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "        data = df_melted[df_melted['metric'] == metric]\n",
    "        data = data[data['label'].isin(ordered_labels)]\n",
    "\n",
    "        sns.boxplot(data=data, x='label', y='value', hue='method',\n",
    "                    palette=method_palette, width=0.6, fliersize=0,\n",
    "                    order=ordered_labels, ax=ax)\n",
    "\n",
    "        # Add mean and std error bars per label and method\n",
    "        for i, label in enumerate(ordered_labels):\n",
    "            for j, method in enumerate(['pretrain', 'scratch']):\n",
    "                stats = summary_stats[metric].get((label, method), None)\n",
    "                if stats is None or np.isnan(stats['mean']):\n",
    "                    continue\n",
    "                # Adjust x position for method side-by-side\n",
    "                # default box width ~0.6, hue splits ~0.4 between 2 methods\n",
    "                offset = -0.2 if method == 'pretrain' else 0.2\n",
    "                xpos = i + offset\n",
    "                mean = stats['mean']\n",
    "                std = stats['std']\n",
    "\n",
    "                ax.hlines(mean, xpos - 0.1, xpos + 0.1, colors='black', linestyles='dashed', linewidth=1.2)\n",
    "                ax.errorbar(xpos, mean, yerr=std, fmt='o', color='black',\n",
    "                            capsize=6, elinewidth=2.5, markeredgewidth=2.5, markersize=8)\n",
    "\n",
    "        # Vertical separator between OS and SS\n",
    "        ax.axvline(x=group_split_index - 0.5, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "        # Non-breaking dash for nicer tick labels\n",
    "        safe_labels = [lbl.replace(\"-\", \"\\u2011\") for lbl in ordered_labels]\n",
    "        ax.set_xticks(range(len(ordered_labels)))\n",
    "        ax.set_xticklabels(safe_labels, rotation=15)\n",
    "\n",
    "        ax.set_title(f\"{metric} (after cut)\", fontsize=14, weight='bold')\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.legend(title='Method', loc='upper right', frameon=True, fontsize=14, title_fontsize=14)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save figures\n",
    "        output_pdf = os.path.join(output_dir, f\"aftercut_{metric}.pdf\")\n",
    "        plt.savefig(output_pdf, bbox_inches='tight')\n",
    "        output_png = os.path.join(output_dir, f\"aftercut_{metric}.png\")\n",
    "        plt.savefig(output_png, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    return summary_stats\n"
   ],
   "id": "a88b2545b58a07ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "summary_stats = plot_aftercut_statistics_separate(boostrap_summary, figsize=(12,12))",
   "id": "9ab9c0d988efa981"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "boostrap_summary_significance = dict()\n",
    "for iboostrap in range(args.boostrap):\n",
    "    boostrap_summary_significance[iboostrap] = dict()\n",
    "    config_workflow_file_pretrain = os.path.join(base_dir_pretrain, f\"control-boostrap-{iboostrap}.yaml\")\n",
    "    config_workflow_file_scratch = os.path.join(base_dir_scratch,  f\"control-boostrap-{iboostrap}.yaml\")\n",
    "    with open(config_workflow_file_pretrain, 'r') as f_boost:\n",
    "        control = yaml.safe_load(f_boost)\n",
    "        plotdir = control[\"output\"][\"plotdir\"]\n",
    "\n",
    "        trainOS_dir = os.path.join(plotdir, \"step4_bump_hunting\")\n",
    "        trainSS_dir = os.path.join(clean_and_append(plotdir, \"_no_signal\"), \"step4_bump_hunting\")\n",
    "\n",
    "        summary_trainOS_testOS_json = os.path.join(trainOS_dir, \"summary_data.json\")\n",
    "        summary_trainOS_testSS_json = os.path.join(trainOS_dir, \"summary_data_no_signal.json\")\n",
    "        summary_trainSS_testSS_json = os.path.join(trainSS_dir, \"summary_data.json\")\n",
    "        summary_trainSS_testOS_json = os.path.join(trainSS_dir, \"summary_data_no_signal.json\")\n",
    "\n",
    "        with open(summary_trainOS_testOS_json, 'r') as f_boost_test:\n",
    "            summary_trainOS_testOS = json.load(f_boost_test)\n",
    "        with open(summary_trainSS_testOS_json, 'r') as f_boost_test:\n",
    "            summary_trainSS_testOS = json.load(f_boost_test)\n",
    "        with open(summary_trainSS_testSS_json, 'r') as f_boost_test:\n",
    "            summary_trainSS_testSS = json.load(f_boost_test)\n",
    "        with open(summary_trainOS_testSS_json, 'r') as f_boost_test:\n",
    "            summary_trainOS_testSS = json.load(f_boost_test)\n",
    "\n",
    "        for nbin, summary in summary_trainOS_testSS.items():\n",
    "            boostrap_summary_significance[iboostrap][nbin] = dict()\n",
    "            for degree, metric in summary.items():\n",
    "                boostrap_summary_significance[iboostrap][nbin][degree] = {'pretrain-trainOS_testOS': summary_trainOS_testOS[nbin][degree]['bonus_significance'],\n",
    "                    'pretrain-trainOS_testSS': summary_trainOS_testSS[nbin][degree]['bonus_significance'],\n",
    "                    'pretrain-trainSS_testOS': summary_trainSS_testOS[nbin][degree]['bonus_significance'],\n",
    "                    'pretrain-trainSS_testSS': summary_trainSS_testSS[nbin][degree]['bonus_significance']\n",
    "                    }\n",
    "\n",
    "    with open(config_workflow_file_scratch, 'r') as f_boost:\n",
    "        control = yaml.safe_load(f_boost)\n",
    "        plotdir = control[\"output\"][\"plotdir\"]\n",
    "\n",
    "        trainOS_dir = os.path.join(plotdir, \"step4_bump_hunting\")\n",
    "        trainSS_dir = os.path.join(clean_and_append(plotdir, \"_no_signal\"), \"step4_bump_hunting\")\n",
    "\n",
    "        summary_trainOS_testOS_json = os.path.join(trainOS_dir, \"summary_data.json\")\n",
    "        summary_trainOS_testSS_json = os.path.join(trainOS_dir, \"summary_data_no_signal.json\")\n",
    "        summary_trainSS_testSS_json = os.path.join(trainSS_dir, \"summary_data.json\")\n",
    "        summary_trainSS_testOS_json = os.path.join(trainSS_dir, \"summary_data_no_signal.json\")\n",
    "\n",
    "        with open(summary_trainOS_testOS_json, 'r') as f_boost_test:\n",
    "            summary_trainOS_testOS = json.load(f_boost_test)\n",
    "        with open(summary_trainSS_testOS_json, 'r') as f_boost_test:\n",
    "            summary_trainSS_testOS = json.load(f_boost_test)\n",
    "        with open(summary_trainSS_testSS_json, 'r') as f_boost_test:\n",
    "            summary_trainSS_testSS = json.load(f_boost_test)\n",
    "        with open(summary_trainOS_testSS_json, 'r') as f_boost_test:\n",
    "            summary_trainOS_testSS = json.load(f_boost_test)\n",
    "\n",
    "        for nbin, summary in summary_trainOS_testSS.items():\n",
    "            for degree, metric in summary.items():\n",
    "                boostrap_summary_significance[iboostrap][nbin][degree].update({'scratch-trainOS_testOS': summary_trainOS_testOS[nbin][degree]['bonus_significance'],\n",
    "                    'scratch-trainOS_testSS': summary_trainOS_testSS[nbin][degree]['bonus_significance'],\n",
    "                    'scratch-trainSS_testOS': summary_trainSS_testOS[nbin][degree]['bonus_significance'],\n",
    "                    'scratch-trainSS_testSS': summary_trainSS_testSS[nbin][degree]['bonus_significance']\n",
    "                    })"
   ],
   "id": "b48afeba7eedc46b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def plot_significance_comparison_per_nbin_degree(boostrap_summary_significance, figsize=(8, 6), output_dir=\"plots_significance_grouped\"):\n",
    "    \"\"\"\n",
    "    Generate significance comparison plots for each (nbin, degree),\n",
    "    grouped by test/train combo (e.g. trainOS_testOS), comparing pretrain vs scratch.\n",
    "    Shows mean ± std and saves plots to PDF.\n",
    "\n",
    "    Parameters:\n",
    "    - boostrap_summary_significance (dict): Dict like:\n",
    "        boostrap_summary_significance[iboostrap][nbin][degree][label] = float\n",
    "    - figsize (tuple): Size of each individual plot.\n",
    "    - output_dir (str): Directory to save output PDFs.\n",
    "\n",
    "    Returns:\n",
    "    - summary_stats (dict): Nested mean/std stats per (nbin, degree, test/train group)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Flatten into long-form DataFrame\n",
    "    records = []\n",
    "    for ibootstrap, per_bootstrap in boostrap_summary_significance.items():\n",
    "        for nbin, per_nbin in per_bootstrap.items():\n",
    "            for degree, per_degree in per_nbin.items():\n",
    "                for label, value in per_degree.items():\n",
    "                    method, group = label.split(\"-\", 1)\n",
    "                    records.append({\n",
    "                        \"bootstrap\": ibootstrap,\n",
    "                        \"nbin\": nbin,\n",
    "                        \"degree\": degree,\n",
    "                        \"method\": method,\n",
    "                        \"group\": group,\n",
    "                        \"significance\": value\n",
    "                    })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # Sort and group by (nbin, degree)\n",
    "    summary_stats = {}\n",
    "    group_order = sorted(df['group'].unique(), key=lambda x: ('OS' in x, x))\n",
    "    method_palette = {'pretrain': '#1f77b4', 'scratch': '#ff7f0e'}\n",
    "\n",
    "    for (nbin, degree), group_df in df.groupby(['nbin', 'degree']):\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        summary_stats[(nbin, degree)] = {}\n",
    "\n",
    "        sns.boxplot(\n",
    "            data=group_df,\n",
    "            x='group', y='significance', hue='method',\n",
    "            palette=method_palette,\n",
    "            order=group_order,\n",
    "            width=0.6, fliersize=0, ax=ax\n",
    "        )\n",
    "\n",
    "        for i, group in enumerate(group_order):\n",
    "            for j, method in enumerate(['pretrain', 'scratch']):\n",
    "                values = group_df[(group_df['group'] == group) & (group_df['method'] == method)]['significance']\n",
    "                if values.empty:\n",
    "                    continue\n",
    "                mean = values.mean()\n",
    "                std = values.std(ddof=1)\n",
    "                x_pos = i + (-0.2 if method == 'pretrain' else 0.2)\n",
    "                ax.errorbar(\n",
    "                    x_pos, mean, yerr=std,\n",
    "                    fmt='o', color='black', capsize=5, elinewidth=2, markersize=6,\n",
    "                    zorder=5,\n",
    "                )\n",
    "                ax.text(\n",
    "                    x_pos, mean + std + 0.1,  # padding above error bar\n",
    "                    f\"{mean:.2f} ± {std:.2f}\",\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=9,\n",
    "                    color='black',\n",
    "                    rotation=0,  # <-- horizontal text\n",
    "                    zorder=6,\n",
    "                    bbox=dict(\n",
    "                        boxstyle='round,pad=0.3',  # rounded box with some padding\n",
    "                        facecolor='white',         # background color\n",
    "                        alpha=0.5,                 # transparency (0=fully transparent, 1=opaque)\n",
    "                        edgecolor='none'           # no border\n",
    "                    )\n",
    "                )\n",
    "                summary_stats[(nbin, degree)][f\"{method}-{group}\"] = {\n",
    "                    'mean': float(mean), 'std': float(std)\n",
    "                }\n",
    "\n",
    "        # Add horizontal line at y=6.4 with label on the right\n",
    "        ax.axhline(6.4, color= \"#DC143C\", linestyle='--', linewidth=1.5, alpha=0.75)\n",
    "\n",
    "        # Place label on top of the line, near right side inside the plot area\n",
    "        ax.text(\n",
    "            len(group_order) - 1.4, 6.2, r\"6.4$\\sigma$ (2502.14036)\",\n",
    "            color=\"#DC143C\", fontsize=10,\n",
    "            va='center', ha='left',  # vertically centered on line\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=0.5)\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"Likelihood Reweighted Significance\", fontsize=14)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"Significance\")\n",
    "        ax.set_xticklabels([g.replace(\"_\", \"\\n\") for g in group_order], fontsize=10)\n",
    "        ax.axhline(0, linestyle='--', color='gray', linewidth=1)\n",
    "        ax.legend(title='Method', loc='upper right', fontsize=10, title_fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        pdf_path = os.path.join(output_dir, f\"significance_bin{nbin}_degree{degree}.pdf\")\n",
    "        plt.savefig(pdf_path)\n",
    "        pdf_path = os.path.join(output_dir, f\"significance_bin{nbin}_degree{degree}.png\")\n",
    "        plt.savefig(pdf_path)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    return summary_stats\n",
    "\n"
   ],
   "id": "f2d03cb7fe460263"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_significance_comparison_per_nbin_degree(boostrap_summary_significance)",
   "id": "91e4b2e68d2592a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c8e2a59a43bd9bd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f73bb5b32fe60b75"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
